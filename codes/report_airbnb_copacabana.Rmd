---
title: "DA3 Assignment 1 - Airbnb Rio Analysis"
author: "Cosmin Catalin Ticu"
date: "02/02/2021"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Clear memory
rm(list=ls())
# Descriptive statistics and regressions
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(stargazer)
library(xtable)
library(directlabels)
library(knitr)
library(cowplot)
library(rattle)
library(ranger)
library(Hmisc)
library(kableExtra)

# set data dir, load theme and functions
source("F:/OneDrive - Central European University/Courses/Winter_Term/DA3/da3_airbnb_price-prediction/codes/gabor_textbook/theme_bg.R")
source("F:/OneDrive - Central European University/Courses/Winter_Term/DA3/da3_airbnb_price-prediction/codes/gabor_textbook/da_helper_functions.R")

#location folders
# Used area
area <- "copacabana"
data <-
  read_csv("F:/OneDrive - Central European University/Courses/Winter_Term/DA3/da3_airbnb_price-prediction/data/clean/airbnb_copacabana_workfile_adj.csv") %>%
  mutate_if(is.character, factor)

options(digits = 3)
```

# Executive Summary

The aim of this study (found in its entirety in this [GitHub repository](https://github.com/cosmin-ticu/da3_airbnb_price-prediction)) is to help a company operating small and mid-sized apartments to price their properties outside of the real estate market. The task at hand comprises extracting Airbnb data (raw data can be found [here](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/blob/main/data/raw/listings_rio_dec-2020.csv)) for the city of choice, as Airbnbs tend to follow their own pricing market, and using the extracted data to estimate the price of a property according to facilities, environment and a host of other characteristics. As such, this data prediction task implies building and contrasting between Airbnb price predicting models in the city of Rio de Janeiro, Brazil, in order to find the best combination of parameters and algorithms. The results (outputs & graphs can be found [here](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/tree/main/output(s))) of this study showcase algorithmic complexity, black-box models' superior performance and the accuracy effects of highly complicated predictions. While an OLS regression is most often enough to investigate the patterns of association between a few variables with regards to their coefficients, this study's usage of the random forest modeling choice found that in order to achieve more confident overall prediction results, black-box models should be used. The final model of choice comprises many factors such as individual amenities (very granular level), host reliability characteristics and property facilities, proving that that simplest models might not always be the best choice.

# Introduction - Purpose & Rationale

With a diverse portfolio of property choices throughout the world, Airbnb represents the largest short-term rental agency. The company supporting this study is interested in learning how to price their small and mid-sized apartments on an alternative market to the real estate one. For the purpose of this study, this task is understood to refer to short-term rental, which can be incrementally more profitable than a direct sale of real estate properties.

The city of choice for this analysis is Rio de Janeiro in Brazil, a hub in South America and notorious for a large socio-economic gap for its residents. This makes the city a perfect contender for a price predicting model for small and mid-sized apartments. The performance, accuracy and efficiency of the predictive models will be put under strain trying to handle the large differences in prices, facilities and amenities. With a potential monopoly from certain Airbnb hosts, it is worthwhile to explore factors that pertain to the hosts, as Airbnb has many verification and rating mechanisms in place for the owners of the listings. As such, considering the large pool of factors that go into the price estimation of a short-term rental, this study will attempt to include as many characteristics, both Airbnb specific (through review counts and host characteristics) and general (number of accommodates, bathrooms, etc.).

The analysis undertaken by this study is similar in nature to the Airbnb case study conducted by Bekes & Kezdi in their ["Data Analysis for Business, Economics and Policy"](https://gabors-data-analysis.com/) book (follow chapters 14 & 16 for reference). Part of the aim of this report is to contrast the findings and modeling choices of Bekes & Kezdi for their London Airbnb dataset. As such, a similar research question to the guiding textbook arises: How can we predict the Airbnb daily price of a small or mid-sized apartment for the city of Rio de Janeiro? Accordingly, what are the important features that go into predicting the daily price of such an accommodation? Furthermore, how do different predictive models perform and uncover the patterns of association?

This study can be classified as a prediction exercise with cross-sectional data. Our purpose is to predict prices for many different apartments throughout the city of Rio. Nonetheless, the sheer complexity of the dataset (which will be discussed in detail later) implies that a subsample within Rio de Janeiro might be the better choice. For the purpose of testing external validity (at a later stage) and for narrowing down the already-complex nature of this study, the data and guiding questions will be entirely restricted to the most popular neighborhood of Rio, Copacabana. Naturally, for external validity's sake, we have to rely on the assumptions that prices do not fluctuate drastically for other neighborhoods and that they stay largely consistent throughout the year. This matter will be touched up within the closing arguments of this paper.

The structure of this report takes a standard data science approach. First, an introduction into the dataset, variables, functional forms and summary statistics will be provided. These details will allow us to make concious decisions about what to include in the analysis and how to predict the price. Following this data cleaning, exploration and fine-tuning, a regression model building section will cover all the predictive modeling choices undertaken within the study. The regression models of various complexity will be compared and the most performing one will be used to compute a few diagnostic statistics for our price prediction. Upon this dignostication, a section on LASSO regression will outline the main difference between the regression model of choice, created with arbitrarily (based on some degree of domain knowledge) chosen variables, and the algorithmically chosen variables of the LASSO model. Lastly, the same process as for the regression models will be adopted for a random forest predictive model. This model will be evaluated through diagnostics and its black-box prediction will be discussed. To answer the latter part of the research question, a variable importance analysis will be conducted as the final step of this study.

# Data & Variables

The data for this case study stems from "[Inside Airbnb](http://insideairbnb.com/)", a website providing fairly cleaned webscrapped data on listings throughout all the major cities of the world. The dataset of choice represents a cross-section of Rio de Jainero listings scrapped from the web on the 23rd of December, 2020 (accessible [here](http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2020-12-23/data/listings.csv.gz). The target variable is price per night, expressed in the local currency of Brazilian reals. Before discussing other variables, it is worthwhile to inspect the distribution of the price, once a few extreme values (above 6000 Brazilian reals, or 1000 EUR) have been cleaned. This cleaning was done in order to exclude some of the extremely luxurious listings as well as some of the potential typos in price (over 10000 EUR/night for an apartment accommodating 6 people seems a little extreme, right?).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Distribution of price by type

# Histograms
# price
price_density <- ggplot(data=data, aes(x=price)) +
  geom_histogram_da(type="percent", binwidth = 50) +
  #geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 10, boundary=0,
  #               color = color.outline, fill = color[1], size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
#  coord_cartesian(xlim = c(0, 400)) +
  labs(x = "Price (Brazilian reals)",y = "Percent")+
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.03), labels = scales::percent_format(1)) +
    scale_x_continuous(expand = c(0.00,0.00),limits=c(0,6000), breaks = seq(0,6000, 500)) +
  theme_bg()+
  ggtitle('Distribution of apartment prices in Copacabana (level scale)')
price_density

# lnprice
ln_price_density<- ggplot(data=data, aes(x=ln_price)) +
  geom_histogram_da(type="percent", binwidth = 0.2) +
  #  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.18,
  #               color = color.outline, fill = color[1], size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
  coord_cartesian(xlim = c(3.5, 8.7)) +
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.05), labels = scales::percent_format(5L)) +
  scale_x_continuous(expand = c(0.00,0.01),breaks = seq(3.3,8.9, 0.6)) +
  labs(x = "ln(price, Brazilian reals)",y = "Percent")+
  theme_bg()+
  ggtitle('Distribution of apartment prices in Copacabana (log scale)')
ln_price_density
```

Price data will almost always be skewed with a long right tail, even after removing all of the truly extreme values. For this reason, the logarithmic transformation of price was done. With a normal distribution, the natural log of the price would be the modeling choice here. However, for the scope of easy interpretation and with respect to the predictive scope of the study (not causal), we can proceed using the price variable as level.

With regards to the other variables, the number of people accommodated was restricted to between 2 and 6 in order to match the client's criteria for small to mid-sized apartments. This choice removes any extreme values from our dataset, such as extremely large apartments or houses, or listings that classify as an apartment yet can only accommodate 1 person. This modeling choice restricts our sample to a normal (90% of the time) search on Airbnb. The property types were restricted to 4 main categories, as per Airbnb's classification: 'Entire Apartment', 'Entire Condominium', 'Private room in apartment' and 'Private room in condominium'. The private rooms were also chosen as oftentimes there is no difference in amenities, facilities and even price between listings self-described as entire apartments and ones that are self-described as private rooms, even though they both have kitchens and bathrooms. The inclusion of private rooms makes our analysis more general.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Boxplot of price by room type
price_vs_room_box <- ggplot(data = data, aes(x = room_type, y = price)) +
  stat_boxplot(aes(group = room_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = room_type),
               color = c(color[2],color[1]), fill = c(color[2],color[1]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,1500), breaks = seq(0,1500,300)) +
  labs(x = "Room type",y = "Price (Brazilian reals)")+
  theme_bg()+
  ggtitle('Distribution of prices in Copacabana by room type')
price_vs_room_box

# Boxplot
g5 <- ggplot(data, aes(x = factor(accommodates), y = price,
                        fill = factor(property_type), color=factor(property_type))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
    stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
    scale_color_manual(name="",
                     values=c(color[2],color[1],color[3],color[4])) +
  scale_fill_manual(name="",
                     values=c(color[2],color[1],color[3],color[4])) +
  labs(x = "Accomodates (Persons)",y = "Price (Brazilian reals)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 2500), breaks = seq(0,2500, 500))+
  theme_bg() +
  theme(legend.position = c(0.1,0.8)       )+
  ggtitle('Distribution of apartment prices in Copacabana by property type and number of accommodated people')
g5
```

The rest of the variables chosen span host characteristics (superhost designation, acceptance rate, response rate, listings count, profile pic and identity verification), general room characteristics (bathrooms, bedrooms, beds), characteristics of the stay (minimum nights required to book), Airbnb review matters (number of reviews and review scores overall rating), and lastly, amenities (spanning an extremely long of list specific amenities ranging from: pool, parking garage, BBQ until shampoo, towels and TV).

## Data Cleaning, Selection & Functional Forms

The data was first cleaned by adopting 4 different methods of handling missing values, each according to the importance of the designated variables. First of all, all the instances where the price was missing were dropped from the dataset. This was done because we could not conceptually impute data for our target variable. Before any other cleaning and functional form creation, which we all non-mandatory steps (as the regression models could have been built without these techniques), our dataset spanned 5632 observations, each representing a single listing. The second round of handling missing values centered around imputing values for the columns that did not contain too many missing observations (usually less than 10% of the overall data). In this case, when missing, the number of bathrooms was replaced with the median values, the number of beds was replaced according to the assumption that 1 bed usually corresponds to about 1.5 guests (this translates better as 2 beds per every 3 guests - a logical assumption & fitting domain knowledge) and the number of bedrooms was replaced accordiing to the assumption that 1 bedroom usually corresponds to 2 guests. All of the individual score component variables were removed from the dataset as they contained too many missing variables. This leads us to the last technique for handling missing values, which is to impute a value even when the proportion of missing data is large, but to also add a flag variable for every observation that was artifically imputed. The variables impacted by this technique are review scores rating, host acceptance rate and the host response rate. These variable were deemed to important to conduct the analysis without (and we will later see that they bring up the predictive models' performance).

For the variable transformation and functional form choices, the categorical variables (property type & room type) were transformed into factor dummies and the rest of the numeric variables went through a distribution check-up.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
################################################
# look at some cnts. key vars, functional form #
################################################

## accomodates: look at distribution

price_vs_accommodates <- ggplot(data = data, aes(x=accommodates, y=price)) +
  geom_point(size=1, colour=color[3], shape=16)+
  ylim(0,6000)+
  xlim(0,8)+
  labs(x="Number of people accomodated",y="Price")+
  geom_smooth(method="lm", colour=color[1], se=FALSE)+
  theme_bg()+
  ggtitle('Distribution of apartment prices in Copacabana by number of accommodated people')
price_vs_accommodates
```

The number of accommodates variable was left as is, without any transformation to logarithmic scale. This is a different modeling choice than presented by Bekes & Kezdi as their dataset contained a much wider accommodation spectrum, thus potentially skeweing the data. In our case, the above dispersion looks quite normal, naturally validating the assumption that apartments that can house more people tend to cost more. The beds variable was converted to a logarithmic scale, as the distribution of beds has a long right tail (perhaps due to the luxurious apartments that offer 6 or even more beds for a proportionately small number of accommodated people). The number of bathrooms was converted to a factor variable, corresponding to 3 different categories (0, 1 or 2+ bathrooms) in order to reduce the right skew in the bathroom variable.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Number of reviews
nreview_plot <- data %>%
  filter(number_of_reviews < 100)

ggplot(nreview_plot, aes(number_of_reviews)) +
  geom_histogram(binwidth = 5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of reviews") +
  theme_bg()+
  ggtitle('Distribution of number of reviews (cap at 100) for properties in Copacabana')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='50%'}
## review score effect
ggplot(data = data, aes(x=review_scores_rating , y=price)) +
  geom_point(size=1.5, colour=color[3], shape=4) +
  ylim(0,800)+
  xlim(20,100)+
  geom_smooth(method="loess", colour=color[1], se=F)+
  labs(x="Review score",y="Daily price (Brazilian reals)")+
  theme_bg()+
  ggtitle('LOESS fit & distribution of review score by price for properties in Copacabana')

# plot logs of both to see if effect changes
ggplot(data = data, aes(x=log(review_scores_rating) , y=ln_price)) +
  geom_point(size=1.5, colour=color[3], shape=4) +
  geom_smooth(method="loess", colour=color[1], se=F)+
  labs(x="Log Review score",y="Log Daily price (Brazilian reals)")+
  theme_bg()+
  ggtitle('LOESS fit & distribution of log review score by log price for properties in Copacabana')
```

The number of reviews was ultimately converted into a factor variable as well, corresponding to 3 different categories (0, 1-51 or 51+ reviews), in order to minimized the noticeable effects of the right skew (as seen above). The review score variable was kept at a level scale, as its transformation to log did not impact the effect it had on the dependent price variable. Lastly, the minimum number of nights required for a stay and the host's count of listings were converted into factor variables as well. The minimum nights variable was categorized into 1, 2, 3-4 or 5+ nights for the required minimum and the listings count was split into 0, 1, 2-4, 5-9 or 10+ listings. Both of these transformations were made based on a few trials to see if a normal distribution could be achieved between all of the categories, making sure that we do not create extreme values artificially. 

As for the cleaning of the amenities column, which was initially found as a list of strings for each listing, a function adapted from Fasih Atif (whose solution on his GitHub repository can be found [here](https://github.com/fasihatif/Data-Analysis-1-2-3/blob/master/Data_Analysis_3/functions/for_loop_aggregate_columns.R)) went through a pre-set list of amenities (spanning from free parking, pool, BBQ, elevator, shampoo, stove, laundry, etc.) in order to match typos, double occurrences and misspelled amenities. Before running this cleaning function, with each amenity found in all the lists of the 5000+ listings, we were left with roughly 800 different dummy variables, each representing a single amenity. With the cleaning function in place, the duplicated columns were combined and the ones that did not match the search criteria in the function (for example really specific downtown Rio amenities, such as residential parking garage) were only kept if they appeared in at least 5% of all the Copacabana listings. With this cleaning procedure in place, we are left with 60+ columns, each being a dummy variable for an individual popular amenity. A note on bias here would be that all the amenity categories were created by the researcher, potentially omitting traditionally Brazilian elements or extremely luxurious amenities that would definitely drive up the price in real life. This concludes the data cleaning, preparation and functional form selection, leaving us with a working dataset of 5584 observations (individual listings in Copacabana).

# Data Modeling Choices

For the upcoming predictive modeling methodologies (OLS, LASSO & Random Forest), all of the variables were categorized into larger groups in order to pick and mix for the training models and to compare out of sample performance according to model complexity as well (we actually find that the most complicated models containing over 100 variables perform quite well). The basic modeling is done only with the variable of number of accommodated people and gradually builds up in complexity (number of explanatory variables & interactions). The general model contains all the property characteristics (accommodates, beds in log, property type, room type and the factored bathroom variable). The review group of variables contains the flag review rating for imputed values, the review rating and the factored number of reviews. The host characteristics grouping contains all the host matters, from response rate until the factored listings count variable. Lastly, we also add the factored minimum nights variable as well as all the amenities columns. With all the groups in mind, it is worthwhile to first inspect interactions according to domain knowledge to see if there are any significant differences in price between a few pair of factor variables.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#####################
# Setting up models #
#####################

# Basic Variables
basic_lev  <- c("accommodates", "ln_beds", "property_type", "room_type", "f_bathroom")

# Factorized variables
reviews <- c("f_number_of_reviews","review_scores_rating", "flag_review_scores_rating")
host_characteristics <- c("host_response_rate","flag_host_response_rate","host_acceptance_rate","flag_host_acceptance_rate","host_is_superhost","host_has_profile_pic","host_identity_verified", "f_host_listings_count")
nights_add <- "f_minimum_nights"

amenities <- paste(colnames(data[21:87]), sep = ",")

# create dummy vars
dummies <- names(data)[seq(21,87)]
data <- data %>%
  mutate_at(vars(dummies), funs("d"= (.)))
# rename columns
dnames <- data %>%
  select(ends_with("_d")) %>%
  names()
dnames_i <- match(dnames, colnames(data))
colnames(data)[dnames_i] <- paste0("d_", tolower(gsub("[^[:alnum:]_]", "",dummies)))
data <- data %>% select(-dummies)
# Dummy variables: Extras -> collect all options and create dummies
amenities <-  grep("^d_.*", names(data), value = TRUE)

#################################################
# Look for interactions
################################################

#Look up room type interactions
p1 <- price_diff_by_variables2(data, "accommodates", "room_type", "No. of people it accommodates", "Room type") # no interaction needed
p2 <- price_diff_by_variables2(data, "property_type", "room_type", "Property type", "Room type") # no interaction needed
#Look up listings count interactions
p3 <- price_diff_by_variables2(data, "f_host_listings_count", "host_is_superhost", "Host's property listings", "Host is Superhost") # interaction needed - superhost starts to count as of 10+ listings (count as in higher price)
p4 <- price_diff_by_variables2(data, "f_host_listings_count", "host_identity_verified", "Host's property listings", "Host Identity Verified") # interaction needed - cannot ask a high price when no listings and not verif
#Look up number of reviews interactions
p5 <- price_diff_by_variables2(data, "f_number_of_reviews", "host_is_superhost", "Number of reviews", "Host is Superhost") # no interaction
#Look up minimum nights interactions
p6 <- price_diff_by_variables2(data, "f_minimum_nights", "room_type", "Minimum nights stay", "Room type") # no interaction
#Look at property type interactions with diff amenities
p7 <- price_diff_by_variables2(data, "property_type", "d_pool_agg", "Property type", "Listing has a pool") # interaction between pool and having an entire apartment (price goes up)
p8 <- price_diff_by_variables2(data, "property_type", "d_patioorbalcony", "Property type", "Listing has patio or balcony") # no interaction

g_interactions <- plot_grid(p1, p2, p3, p4, nrow = 2, ncol = 2)
g_interactions

g_interactions2 <- plot_grid(p5, p6, p7, p8, nrow = 2, ncol = 2)
g_interactions2
```

According to the plots above the interaction between the host's count of listings and whether they are a superhost is present and can be seen when inspecting hosts with 10 or more listings. As such, being a superhost appears to matter differently with respect to pricing once a host has more than a handful of properties listed on Airbnb. This changing pattern from the hosts with few listings (where on average the superhosts appear to charge less) warrants the inclusion of an interaction term. Furthermore, the interaction between a host's count of listings and whether their identity is verified according to price is also noticeable. This might imply that hosts who do not have any listings and who are also not verified tend to offer lower prices for their property. This appears different than the case of hosts who have 1 property listed, thus warranting the inclusion of an interaction term. The interaction between the pool dummy (having a pool as an amenity at the Airbnb) appears to matter quite a lot for apartments, unlike for other types of listings. This also warrants the addition of an interaction term. Lastly, a series of interaction terms was also added between the room type and number of people accommodated variables and all of the 60+ amenities. This last interaction term will be used for the most advanced model, which will serve as a good basis for the reverse-engineering techniques of the LASSO regression.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# dummies suggested by graphs
X1  <- c("f_host_listings_count*host_is_superhost",  "f_host_listings_count*host_identity_verified")
X2  <- "f_host_listings_count*d_pool_agg"

# Additional interactions of factors and dummies
X3  <- c(paste0("(room_type + accommodates) * (",
                paste(amenities, collapse=" + "),")"))

# Create models in levels models: 1-8
modellev1 <- " ~ accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev, collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev, reviews),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev, host_characteristics, X1),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev, amenities, X3),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev, reviews, nights_add), collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev, reviews, nights_add, host_characteristics, X1),collapse = " + "))
modellev8 <- paste0(" ~ ",paste(c(basic_lev, reviews, host_characteristics, nights_add, X1, X2, amenities, X3),collapse = " + "))

```

## Validation & Holdout

In order to increase the performance of our models and to assess their out of sample final accuracy, this study separates the 5500+ observations into a 20% holdout set and an 80% working set on which the OLS model training can occur. The OLS models are each trained through 5-fold cross-validation so as to find the least overfitting coefficients, taking the average of each model's 5 test folds, warranting the RMSE and BIC statistics with no assumptions. To compare all the models, both the RMSE and BIC are used.

# OLS Predictive Models - Serving as Basis

With the interaction terms also computed, the models of choice under the first predictive methodology (OLS) span different types of complexity (from 1 single explanatory variable to the 8th predictive model containing all of the variables and interactions above). As a note on reading the following tables and graphs, this study aims to follow the same logic as the Bekes & Kezdi case study and differentiates between the number of predictors used (number of variables) and the number of coefficients. This is a common case in data science as models can include interactions or splits of the same variable (thus a single variable can have multiple coefficients).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#################################
# Separate hold-out set #
#################################

# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator: It will make results reproducable
set.seed(20180123)

# create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$holdout <- 0
data$holdout[holdout_ids] <- 1

#Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)

#Working data set
data_work <- data %>% filter(holdout == 0)


##############################
#   cross validation OLS    #
##############################

## N = 5
n_folds=5
# Create the folds
set.seed(20180124)

folds_i <- sample(rep(1:n_folds, length.out = nrow(data_work) ))
# Create results
model_results_cv <- list()


for (i in (1:8)){
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")

  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))

  # Initialize values
  rmse_train <- c()
  rmse_test <- c()

  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared

  # Do the k-fold estimation
  for (k in 1:n_folds) {
    test_i <- which(folds_i == k)
    # Train sample: all except test_i
    data_train <- data_work[-test_i, ]
    # Test sample
    data_test <- data_work[test_i, ]
    # Estimation and prediction
    model <- lm(formula,data = data_train)
    prediction_train <- predict(model, newdata = data_train)
    prediction_test <- predict(model, newdata = data_test)

    # Criteria evaluation
    rmse_train[k] <- mse_lev(prediction_train, data_train[,yvar] %>% pull)**(1/2)
    rmse_test[k] <- mse_lev(prediction_test, data_test[,yvar] %>% pull)**(1/2)

  }

  model_results_cv[[model_name]] <- list(yvar=yvar,xvars=xvars,formula=formula,model_work_data=model_work_data,
                                         rmse_train = rmse_train,rmse_test = rmse_test,BIC = BIC,
                                         model_name = model_pretty_name, nvars = nvars, r2 = r2)
}

model <- lm(formula,data = data_train)
prediction_train <- predict(model, newdata = data_train)
prediction_test <- predict(model, newdata = data_test)

t1 <- imap(model_results_cv,  ~{
  as.data.frame(.x[c("rmse_test", "rmse_train")]) %>%
    dplyr::summarise_all(.funs = mean) %>%
    mutate("model_name" = .y , "model_pretty_name" = .x[["model_name"]] ,
           "nvars" = .x[["nvars"]], "r2" = .x[["r2"]], "BIC" = .x[["BIC"]])
}) %>%
  bind_rows()
column_names <- c("Model", "N predictors", "R-squared", "BIC", "Training RMSE",
                 "Test RMSE")
t14_2 <- t1 %>%
    select("model_pretty_name", "nvars", "r2" , "BIC", "rmse_train", "rmse_test")
colnames(t14_2) <- column_names
knitr::kable(t14_2, caption = "Regression Models Comparison (BIC & R2 on all data; RMSE split according to training & test)")
# -R2, BIC on full work data-n.
# -In sample rmse: average on training data; avg test : average on test data
```

According to the model comparison table above (which can also be found [here as an HTML](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/blob/main/output(s)/regression_models_comparison.html)), the model of choice for this study appears to be model 7, which was built including the host characteritics variable group, the review characteristics variable group, the property characteristics variable group, the minimum nights variable and the interaction terms between some of the host variables. This model balances between a low RMSE, a low BIC and a fairly high R-squared, bearing in mind that the latter 2 measures were computed on the full data, which the RMSE values correspond either to in-sample or out-of-sample performance. Overall, combining the indirect evaluation criteria of the BIC (which adds a penalty term for increasing variable complexity, even in large samples) and the direct evaluation criteria of the cross-validated RMSE (prediction lacking assumptions and positive natural bias arising out of evaluating on the same trained data). Accordingly, it is worthwhile to inspect the RMSE training versus test graph according to the number of coefficients in order to see if our choice of model (with 32 coefficients - and 30 variables) produced an overfit result or not (and to subsequently find the overfitting threshold).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# RMSE training vs test graph
t1_levels <- t1 %>%
  dplyr::select("nvars", "rmse_train", "rmse_test") %>%
  gather(var,value, rmse_train:rmse_test) %>%
  mutate(nvars2=nvars+1) %>%
  mutate(var = factor(var, levels = c("rmse_train", "rmse_test"),
                      labels = c("RMSE Training","RMSE Test")))

model_result_plot_levels <- ggplot(data = t1_levels,
                                   aes(x = factor(nvars2), y = value, color=factor(var), group = var)) +
  geom_line(size=1,show.legend=FALSE, na.rm = TRUE) +
  scale_color_manual(name="",
                     values=c(color[2],color[1])) +
  scale_y_continuous(name = "RMSE", limits = c(400, 500), breaks = seq(400,500, 10)) +
  scale_x_discrete( name = "Number of coefficients", expand=c(0.01, 0.01)) +
  geom_dl(aes(label = var),  method = list("last.points", dl.trans(x=x-1), cex=0.4)) +
  #scale_colour_discrete(guide = 'none') +
  theme_bg()+
  ggtitle('Training V Test RMSE by No. of predictors')
model_result_plot_levels
```

## OLS Interpretation of fit

From the above plot of RMSE values, we can see that once the amenities (as a bulk) are added into the model, the test and training RMSE start to steer away from each other, pointing in the direction of an overfitting model. However, what is highly interesting to observe is that the test RMSE appears to start decreasing once again at the same rate as the training RMSE when a lot more variables (and subsequently coeffcients) are added (the threshold appears somewhere around 170 coefficients). The finding might suggest that some of the amenities (and their subsequent interactions with the main predictors) might bring a significant predictive advantage when incorporated into the model. However, it is clear that not all of the aminities variables are useful (unlike the naive assumption ultimately made by model 8 which incorporated everything assuming equal rights). 

This paves the way for building a LASSO predictive model, which uses a penalty term within its algorithm to select variables according to whether their inclusion improves the overall fit, thus warranting the insignificant variables coefficients that are null. The LASSO regression will automatically narrow down from our model with the largest complexity (model 8) in order to find the optimal number of predictors. We are able to judge its performance according to the produced out-of-sample RMSE.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#################################
#           LASSO               #
#################################

# take model 8
vars_model_8 <- c("price", basic_lev, reviews, host_characteristics, nights_add, X1, X2, amenities, X3)

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

# We use model 7 without the interactions so that it is easy to compare later to post lasso ols
formula <- formula(paste0("price ~ ", paste(setdiff(vars_model_8, "price"), collapse = " + ")))

set.seed(1234)
lasso_model <- caret::train(formula,
                      data = data_work,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                    na.action=na.exclude)

lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `1`)  # the column has a name "1", to be renamed

# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
# best choice would be to pick the LASSO output as it has the best RMSE out of all
```

# LASSO Regression

With a final choice of 149 coefficients, the LASSO regression has managed to produce a test sample RMSE on 5-fold cross-validation of 435, which is significantly lower than even our most complex (200+ coefficients) model (8), using a lambda penalty function of 1 (the upper boundary of the pre-defined spectrum, thus implying an aggressive algorithmic cutting down of coefficients). Even though the LASSO out-of-sample RMSE is better than our most robust OLS model, it might still be worthwhile to choose model 7 for the sheer ease of interpretation. Of course, model 7 does not contain any of the amenities, while LASSO even incorporates some of the amenities' interactions on top of all the other variables. Ultimately with LASSO it comes down to whether you are after the best predicting model irrespective of complexity and black-box nature, or if you would like to be able to inspect each individual coefficient without much of a headache. For the scope of this analysis, the client requiring a general understanding of the small to mid-sized apartment market on Airbnb, the 7th regression currently holds the title of the model of choice due to its simpler nature (Occam's Razor). It is thus worthwhile to run a few model diagnostics on our OLS choice, such as inspecting the fitting versus actual values as well as inspecting some prediction and confidence intervals.

# OLS Diagnostics

```{r, echo=FALSE, warning=FALSE, message=FALSE}

model7_level <- model_results_cv[["modellev7"]][["model_work_data"]]

# look at holdout RMSE
model7_level_work_rmse <- mse_lev(predict(model7_level, newdata = data_work), data_work[,"price"] %>% pull)**(1/2)
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)

###################################################
# FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
###################################################

# Target variable
Ylev <- data_holdout[["price"]]

meanY <-mean(Ylev)
sdY <- sd(Ylev)
meanY_m2SE <- meanY -1.96 * sdY
meanY_p2SE <- meanY + 1.96 * sdY
Y5p <- quantile(Ylev, 0.05, na.rm=TRUE)
Y95p <- quantile(Ylev, 0.95, na.rm=TRUE)

# Predicted values
predictionlev_holdout_pred <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="predict")) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="confidence")) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])


# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price using non-extreme values -> fit of model for "normal" prices
level_vs_pred_normal <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 1000, yend =1000), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 1000), breaks=seq(0, 1000, by=100)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 1000), breaks=seq(0, 1000, by=100)) +
  labs(y = "Price (Brazilian reals)", x = "Predicted price (Brazilian reals)") +
  theme_bg()+
  ggtitle('OLS M7 - Fitted V Actual Price limit 1000 Brazilian reals') 
level_vs_pred_normal


```

To summarize the modeling results, having run 5-fold cross-validation across a spectrum of complexity-differing linear models and one LASSO model, we have picked an OLS model with 32 coefficients (or 30 variables). The graph above containing the fitted versus actual values for price shows us the largely decent fit of our OLS model. However, unlike the fit observed by Bekes & Kezdi in their Airbnb case study of London,the dispersion of values is much larger. At a first glance (at the graph above), with a limit of 1000 Brazilian reals (about 150 EUR) for the price per night, thus excluding the extreme values, our dispersion appears borderline random between the fitted and predicted values. It is worthwhile to inspect the full graph to understand the magnitude of this dispersion.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Plot predicted vs price also using extreme values -> fit of model for all prices
level_vs_pred_all <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 5000, yend =5000), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 5000), ylim = c(0, 5000)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 5000), breaks=seq(0, 5000, by=500)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 5000), breaks=seq(0, 5000, by=500)) +
  labs(y = "Price (Brazilian reals)", x = "Predicted price  (Brazilian reals)") +
  theme_bg()+
  ggtitle('OLS M7 - Fitted V Actual Price original limit of 5000 Brazilian reals') 
level_vs_pred_all
```

The unlimited y-y plot above shows us, just like in the Bekes & Kezdi case study, that our prediction does a better job for much lower prices than for higher ones. When the range of values is as large as 6000 Brazilian reals (about 1000 EUR), we can see that our model never actually estimates about 2000 Brazilian reals, thus completely mislabeling the most expensive of small to mid-sized apartments. This is not necessarily the fault of regression, as majority voting plays a big role here, while extreme values (when far and few) tend to be predicted poorly. Overall, with this fit, we can aim to predict with an algorithm that is better suited for extreme values, random forest (coming up in the next section).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Redo predicted values at 80% PI
predictionlev_holdout_pred <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="predict", level=0.8)) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="confidence", level=0.8)) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])

predictionlev_holdout_summary <-
  predictionlev_holdout %>%
  group_by(accommodates) %>%
  dplyr::summarise(fit = mean(fit, na.rm=TRUE), pred_lwr = mean(pred_lwr, na.rm=TRUE), pred_upr = mean(pred_upr, na.rm=TRUE),
            conf_lwr = mean(conf_lwr, na.rm=TRUE), conf_upr = mean(conf_upr, na.rm=TRUE))

# Plot mean predicted price according to number of accommodates; visualizing 80% PI

PI_80_accommodates <- ggplot(predictionlev_holdout_summary, aes(x=factor(accommodates))) +
  geom_bar(aes(y = fit ), stat="identity",  fill = color[1], alpha=0.7 ) +
  geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
  #geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
  scale_y_continuous(name = "Predicted price (Brazilian reals)") +
  scale_x_discrete(name = "Accomodates (Persons)") +
  scale_color_manual(values=c(color[2], color[2])) +
  theme_bg() +
  theme(legend.title= element_blank(),legend.position="none")+
  ggtitle('OLS M7 - Fitted Price Prediction Interval (PI) by number of accommodates') 
PI_80_accommodates
```

The 80% prediction interval (PI) and confidence interval (CI) statistics according to the number of people accommodated of the chosen OLS model can be found [here](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/blob/main/output(s)/modellev7_holdout_summary.html) to be inspected in detail. However, our main takeaway from this model is that with an extremely wide 80% PI, the average nightly price for a small to mid-sized apartment in Copacabana is 523 Brazilian reals (or about 80 EUR). The prediction uncertainty is extremely large when we consider that our prediction interval estimates that prices may vary between -65 Brazilian reals and 1111 Brazilian reals.

As a last graphical diagnostic measure for the OLS, we can inspect the above graph showing the price prediction interval (set at a PI of 80%, not 95%) for each of the number of people accommodated in a small to mid-sized apartment. As we can clearly see, even with an 80% PI, our model, unlike the Bekes & Kezdi model, produces extremely large prediction intervals, for the 2, 3 and 4 people accommodated even spanning negative values in their lower boundaries. 

The conclusion follows suit in the Bekes & Kezdi case study, showcasing that our prediction has substantial uncertainty, even in all of the subgroups (such as across different number of guests). This concludes the OLS predictive modeling part of this study and paves the way for Random Forest to create a challenger model.

# Random Forest - Building up on OLS

The Random Forest predictive model takes a lot of inspiration from the OLS models, building up on their formulas. The random forest modeling approach also uses a 5-fold cross-validation method and predicts the final values used in model diagnostics on the same holdout set that was defined at the beginning of the OLS modeling section. The first random forest model uses all of model 7's predictors (basic property characteristics, review matters, minimum nights and host characteristics) while the second model uses all of model 8's predictors (same as model 7 but with the inclusion of all the amenity dummies). The only difference to the OLS models is that random forest does not use any interactions, thus having as many predictors as there are "coefficients" (even though here they cannot be considered coefficients, but rather splitting variables). The third model of random forest is an automatically tuning model (self-defines bootstrap samples, mtry and minimum node size) which consumes the most computational power, but does not require any tuning parameters to be manually input.

This study follows suit from Bekes & Kezdi's RF tuning parameter choices, sticking to either a 3-by-2 grid or a 3-by-3 grid for defining the mtry tuning parameter (the number of variables considered for each split) and the minimum number of observations in the terminal nodes of each tree as a stopping rule. The number of bootstrap samples is set to the default rule of 500, available in the respective R package.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#########################################################################################
#
# RANDOM FORESTS -------------------------------------------------------
#
#########################################################################################

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(5, 7, 9),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

# Define the two sets of predictors for Random Forest without interactions
predictors_1 <- c(basic_lev, reviews, nights_add, host_characteristics) # equivalent of model 7 without interactions
predictors_2 <- c(basic_lev, reviews, host_characteristics, nights_add, amenities) # equivalent of model 8 without interactions


# simpler model for model A (1)
set.seed(1234)
  rf_model_1 <- train(
    formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )

# set tuning for benchamrk model (2)
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

set.seed(1234)
  rf_model_2 <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )

# auto tuning first
set.seed(1234)
  rf_model_2auto <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity"
  )


# evaluate random forests -------------------------------------------------

results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2,
    model_2b = rf_model_2auto
    
  )
)

# Show outputs -------------------------------------------------------

# Tuning parameter choice 1
result_2 <- matrix(c(mean(results$values$`model_1~RMSE`),
                     mean(results$values$`model_2~RMSE`),
                     mean(results$values$`model_2b~RMSE`)
),
nrow=3, ncol=1,
dimnames = list(c("Model A", "Model B","Model B auto"),
                c(results$metrics[2]))
)


knitr::kable(x = as.data.frame(result_2), caption = "RF Models' RMSE")
```

Based on the results from all of the random forest models (as seen above and in this table [here](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/blob/main/output(s)/rf_models_rmse.html)), the best model of choice is not the autotuning one, but rather the most complex random forest, containing all of the variables used in model 8 of the OLS regression. With a test RMSE of 407, it has achieved a significantly better result than even the LASSO black-box regression.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Tuning parameter choice 2
result_1 <- matrix(c(
  rf_model_1$finalModel$mtry,
  rf_model_2$finalModel$mtry,
  rf_model_2auto$finalModel$mtry,
  rf_model_1$finalModel$min.node.size,
  rf_model_2$finalModel$min.node.size,
  rf_model_2auto$finalModel$min.node.size
  
),
nrow=3, ncol=2,
dimnames = list(c("Model A", "Model B","Model B auto"),
                c("Min vars","Min nodes"))
)
knitr::kable(x = as.data.frame(result_1), caption = "Best tuning choices for each RF model")
```

The above table showcases the best tuning parameters for each of the RF models. An interesting finding here which also speaks for the computational complexity of the autotuning model (taking over 90 seconds to run on this relatively small dataset). The other two models, having much lower numbers of variables considered for each split were able to complete the predictive model in 16 and 30 seconds respectively. Overall, the model holding decent performance with the lowest RMSE, model B (built on the complex OLS model 8 variables), will be used for model diagnostics and, as our best model yet, will allow us to plot all the variables according to importance.

## Random Forest - Diagnostics

The same diagnostics used for OLS are also used for the Random Forest models. In this case, we inspect the y-y plots, one limited to 1000 Brazilian reals, just as in OLS (excluding extremely large values), and one containing the spectrum of the original data, up to 6000 Brazilian reals.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# create the predicted price from random forest model 2 into a new holdout dataframe
# use for fitted versus actual outcome plots

data_holdout_w_rf_prediction <- data_holdout %>%
  mutate(predicted_price = predict(rf_model_2, newdata = data_holdout))

# # FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
# ###################################################

# Plot predicted vs price using non-extreme values -> fit of model for "normal" prices
rf_level_vs_pred_normal <- ggplot(data = data_holdout_w_rf_prediction) +
  geom_point(aes(y=price, x=predicted_price), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 1000, yend =1000), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 1000), breaks=seq(0, 1000, by=100)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 1000), breaks=seq(0, 1000, by=100)) +
  labs(y = "Price (Brazilian reals)", x = "Predicted price  (Brazilian reals)") +
  theme_bg()+
  ggtitle('RF Model B (OLS Model 8 basis) - Fitted V Actual Price limit 1000 Brazilian reals') 
rf_level_vs_pred_normal

# Plot predicted vs price also using extreme values -> fit of model for all prices
rf_level_vs_pred_all <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 5000, yend =5000), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 5000), ylim = c(0, 5000)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 5000), breaks=seq(0, 5000, by=500)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 5000), breaks=seq(0, 5000, by=500)) +
  labs(y = "Price (Brazilian reals)", x = "Predicted price  (Brazilian reals)") +
  theme_bg()+
  ggtitle('RF Model B (OLS Model 8 basis) - Fitted V Actual Price original limit 5000 Brazilian reals') 
rf_level_vs_pred_all
```

From the y-y plot limited to 1000 Brazilian reals we can clearly see a better fit for the Random Forest model. Unlike the OLS model of choice, the RF Model B actually showcases a trend in predicting lower prices quite well. However, just as the OLS model, it still cannot do a good job when dealing with extreme values. However, an improvement can definitely be seen going from the rather straight-forward OLS with only 30 variables to the black-box algorithm of random forest with 100 variables. Overall, it is only a matter of adversity to complexity. Within the scope of this study, with such high improvements stemming from the RF model, the main model of choice for the entire study is the complex self-defined RF Model B (based on the OLS model 8 coefficients). Accordingly, we can proceed with plotting variable importance.

## Random Forest - Variable Importance Plots

Once we have picked the best model to use in our analysis, to understand the patterns of the x-y association, we can create variable importance plots.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#########################################################################################
# Variable Importance Plots -------------------------------------------------------
#########################################################################################
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}


# variable importance plot
# 1) full varimp plot, full
# 2) varimp plot grouped
# 3) varimp plot , top 10
# 4) varimp plot  w copy, top 10


rf_model_2_var_imp <- importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  # mutate(varname = gsub("property_type", "Property Type:", varname) ) %>%
  # mutate(varname = gsub("room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))


##############################
# 1) full varimp plot, above a cutoff
##############################


cutoff = 600
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
                                  aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1.5) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
  ylab("Importance (Percent)") +
  xlab("Variable/Predictor Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
        axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))+
  ggtitle('RF Model B - Predictor Importance Plot')
rf_model_2_var_imp_plot
```

Displayed above, the variable importance plot shows the importance on the horizontal axis as measured in percentage points. Calculation of the importance follows suit from Bekes & Kezdi's case study and book methodology, computing importance by averaging the MSE reduction of each variable across all the trees (within the random forest). Once each variable has a computed importance, to get it in percentage terms, we merely divide divide each variable's averaged MSE reduction across the sum all other variables' averaged MSE reductions. While the above graph shows a few variables with over 2% importance, it is hard to decipher them out of almost the entire spectrum of variables (an importance cutoff was defined for this plot, otherwise it would not have even fit on a page). The graph can also be found [here](https://github.com/cosmin-ticu/da3_airbnb_price-prediction/blob/main/output(s)/rf-varimp-base-plot.png) in better quality.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

##############################
# 2) full varimp plot, top 10 only
##############################


# have a version with top 10 vars only
rf_model_2_var_imp_plot_b <- ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable/Predictor Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
        axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))+
  ggtitle('RF Model B - Predictor Importance Plot - Top 10 predictors')
rf_model_2_var_imp_plot_b
```

A workaround for plotting variable importance within a readable format is to focus only on the top 10 predictors. As we can see above, the graph becomes much more readable and we can see that features like the number of people a listing can accommodate, the number of bathrooms, the host's acceptance rate and the inclusion of a pool in the amenities have a noticeable importance in reducing the RMSE of our model, thus contributing significantly to its accuracy. One issue, however, with this plot is that we are still not able to see the variables, but we are rather observing the predictors in this case (similar to "coefficients" from OLS). Of more interest could be to group all of the factors back into their according variables in order to see the actual variable importance plot.

```{r, echo=FALSE, warning=FALSE, message=FALSE}


##############################
# 3) varimp plot grouped
##############################
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model_2$finalModel$xNames
f_bathroom_varnames <- grep("f_bathroom",varnames, value = TRUE)
f_numer_of_reviews_varnames <- grep("f_number_of_reviews",varnames, value = TRUE)
f_minimum_nights_varnames <- grep("f_minimum_nights",varnames, value = TRUE)
f_property_type_varnames <- grep("property_type",varnames, value = TRUE)
f_room_type_varnames <- grep("room_type",varnames, value = TRUE)
f_host_listings_count_varnames <- grep("f_host_listings_count",varnames, value = TRUE)


groups <- list(f_bathroom=f_bathroom_varnames,
               f_numer_of_reviews = f_numer_of_reviews_varnames,
               f_minimum_nights = f_minimum_nights_varnames,
               f_property_type = f_property_type_varnames,
               f_room_type = f_room_type_varnames,
               f_host_listings_count = f_host_listings_count_varnames,
               review_scores_rating = "review_scores_rating",
               accommodates = "accommodates",
               ln_beds = "ln_beds",
               host_acceptance_rate = 'host_acceptance_rate',
               host_response_rate = 'host_response_rate',
               host_is_superhost = 'host_is_superhost',
               host_identity_verified = 'host_identity_verified',
               pool_dummy = 'd_pool_agg')

rf_model_2_var_imp_grouped <- group.importance(rf_model_2$finalModel, groups)
rf_model_2_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_2_var_imp_grouped),
                                            imp = rf_model_2_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_2_var_imp_grouped_plot <-
  ggplot(rf_model_2_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
        axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))+
  ggtitle('RF Model B - Grouped Variable Importance Plot (original Airbnb variables)')
rf_model_2_var_imp_grouped_plot
```

When pooling all the factor variables back into their original setting, we can observe a large increase in the importance of the count of a host's listings, while variables like property type and room type make their way into the plot. Overall, we see a multitude of features that are conducive to a better Random Forest predictive model. It is worthwhile for the client to consider not only property characteristics, such as the number of bathrooms, but also they way they present and handle themselves as a short-term rental agent (host characteristics matter a lot!), the reviews that they have received in the past, and last but not least, the luxurious amenities that they can offer, such as a pool or a hot tub.

# External Validity

While the scope of this study was restricted to the neighborhood of Copacabana, it is worthwhile to inspect the external validity of this model by applying it either throughout Rio or to another one of its many boroughs. At the end of the day, a model (or series of models as in this case) is only as good as it is far-reaching. Considering the limitations imposed by the data, the highly unequal pricing nature of Airbnb and Rio's large socio-economic inequality gap, we can safely say that we have built a model setting a strong basis for gradient boosting machines, neural networks and a host of other more advanced data science algorithms for prediction.

# Further Research

This case study has showcased the complexity associated with predicting an feature that human intuition is so akin to, the value of an object. Taking into consideration all of the features associated with predicting the price of a short-term small to mid-sized apartment, especially in the extremely large and socio-economically unequal city of Rio de Janeiro, it is not surprising that this study has found a few interesting and important variables to consider. However, the extremely large prediction interval of the OLS model of choice suggests that clients looking to estimate exactly the nightly price they should be listing their property for have not come to the right place. While data analysis helps us uncover the general pattern of association within data (and might even allow us to approximate the direction in which our data might be going), getting accurate predictions even out of the complex black-box models is better left to future advancements in data science or just plainly choosing an overfit model to please your clients.
